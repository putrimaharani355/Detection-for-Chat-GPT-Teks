# -*- coding: utf-8 -*-
"""Skripsi  (Optimizer SGD)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17awOimCtvsLsSjILH_j-BiLVxY-6kVdG

## Lebih dari 200
"""

# Import Library
import pandas as pd
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import tensorflow as tf
from transformers import TFBertForSequenceClassification

import re
df = pd.read_csv('/content/dataset_sampled (1).csv')
def text_preprocessing(text):
    # Mengubah semua karakter menjadi huruf kecil
    text = text.lower()
    text = text.replace('\n', ' ').replace('\r', ' ')
    text = re.sub(r'[^\w\s]', '', text)

    # Menghapus spasi di awal dan akhir teks
    text = text.strip()

    return text


df['text'] = df['text'].apply(lambda x: text_preprocessing(x))

X = df['text']
y = df['class']

# Membagi dataset menjadi data pelatihan dan pengujian (75% train, 25% test)
train_texts, test_texts, train_labels, test_labels = train_test_split(X, y, test_size=0.25, random_state=42)

# Membagi data pelatihan menjadi data pelatihan dan validasi (90% train, 10% validation)
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)

# Menampilkan jumlah data pada masing-masing set
print(f"Jumlah data train: {len(train_texts)}")
print(f"Jumlah data validation: {len(val_texts)}")
print(f"Jumlah data test: {len(test_texts)}")

# Assuming train_texts and test_texts are your pandas Series objects
train_texts_list = train_texts.tolist()
test_texts_list = test_texts.tolist()
val_texts_list = val_texts.tolist()

# Menggunakan tokenizer BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(train_texts_list, truncation=True, padding=True, max_length=512)
test_encodings = tokenizer(test_texts_list, truncation=True, padding=True, max_length=512)
val_encodings = tokenizer(val_texts_list, truncation=True, padding=True, max_length=512)

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels
))
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))

# Memuat model BERT untuk klasifikasi teks (contoh: sentimen positif/negatif)
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

optimizer = tf.keras.optimizers.SGD(learning_rate=3e-5, momentum=0.9)

model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
model.dropout = tf.keras.layers.Dropout(0.2)  # Tingkat dropout yang lebih tinggi
model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy'])

# Early stopping callback
# early_stopping = tf.keras.callbacks.EarlyStopping(
#     monitor='val_loss',  # metric to monitor
#     patience=3,          # number of epochs with no improvement after which training will be stopped
#     restore_best_weights=True  # restore the model weights from the epoch with the best value of the monitored metric
# )

# Training
history = model.fit(
    train_dataset.shuffle(25).batch(8),
    epochs=25,
    validation_data=val_dataset.batch(8),
    # callbacks=[early_stopping]
)

import matplotlib.pyplot as plt
fig , ax = plt.subplots(1,2)
train_acc = history.history['accuracy']
train_loss = history.history['loss']
fig.set_size_inches(12,4)

ax[0].plot(history.history['accuracy'])
ax[0].plot(history.history['val_accuracy'])
ax[0].set_title('Training Accuracy vs Validation Accuracy')
ax[0].set_ylabel('Accuracy')
ax[0].set_xlabel('Epoch')
ax[0].legend(['Train', 'Validation'], loc='upper left')

ax[1].plot(history.history['loss'])
ax[1].plot(history.history['val_loss'])
ax[1].set_title('Training Loss vs Validation Loss')
ax[1].set_ylabel('Loss')
ax[1].set_xlabel('Epoch')
ax[1].legend(['Train', 'Validation'], loc='upper left')

plt.show()

# Evaluasi Model
loss, accuracy = model.evaluate(test_dataset.batch(16))
print("Akurasi:", accuracy)
print("Loss:", loss)

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf

# Prediksi Model
predictions = model.predict(test_dataset.batch(16))
predicted_classes = tf.argmax(predictions.logits, axis=1)

# Membuat confusion matrix
conf_mat = confusion_matrix(test_labels, predicted_classes)

print("Confusion Matrix:")
print(conf_mat)

# Menampilkan laporan klasifikasi
target_names = ['Human generated', 'GPT generated']
print(classification_report(test_labels, predicted_classes, target_names=target_names))

# Membuat heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix Heatmap')
plt.show()

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Misalnya test_texts, test_labels, dan predicted_classes sudah didefinisikan sebelumnya


# Membuat DataFrame
results_df = pd.DataFrame({
    'Text': test_texts,
    'True Label': test_labels,
    'Predicted Label': predicted_classes
})

# Mengubah nilai 0 dan 1 menjadi "human generated" dan "gpt generated"
label_map = {0: "human generated", 1: "gpt generated"}
results_df['True Label'] = results_df['True Label'].map(label_map)
results_df['Predicted Label'] = results_df['Predicted Label'].map(label_map)

results_df.head()

"""## Kurang dari 200"""

# Import Library
import pandas as pd
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import tensorflow as tf
from transformers import TFBertForSequenceClassification

import re
df = pd.read_csv('/content/dataset_sampled_kurang_200 (2).csv')
def text_preprocessing(text):
    # Mengubah semua karakter menjadi huruf kecil
    text = text.lower()
    text = text.replace('\n', ' ').replace('\r', ' ')
    text = re.sub(r'[^\w\s]', '', text)

    # Menghapus spasi di awal dan akhir teks
    text = text.strip()

    return text


df['text'] = df['text'].apply(lambda x: text_preprocessing(x))

X = df['text']
y = df['class']

# Membagi dataset menjadi data pelatihan dan pengujian (75% train, 25% test)
train_texts, test_texts, train_labels, test_labels = train_test_split(X, y, test_size=0.25, random_state=42)

# Membagi data pelatihan menjadi data pelatihan dan validasi (90% train, 10% validation)
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)

# Menampilkan jumlah data pada masing-masing set
print(f"Jumlah data train: {len(train_texts)}")
print(f"Jumlah data validation: {len(val_texts)}")
print(f"Jumlah data test: {len(test_texts)}")

# Assuming train_texts and test_texts are your pandas Series objects
train_texts_list = train_texts.tolist()
test_texts_list = test_texts.tolist()
val_texts_list = val_texts.tolist()

# Menggunakan tokenizer BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(train_texts_list, truncation=True, padding=True, max_length=300)
test_encodings = tokenizer(test_texts_list, truncation=True, padding=True, max_length=300)
val_encodings = tokenizer(val_texts_list, truncation=True, padding=True, max_length=300)

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels
))
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))

# Memuat model BERT untuk klasifikasi teks (contoh: sentimen positif/negatif)
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

optimizer = tf.keras.optimizers.SGD(learning_rate=3e-5, momentum=0.9)

model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
model.dropout = tf.keras.layers.Dropout(0.2)  # Tingkat dropout yang lebih tinggi
model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy'])

# Early stopping callback
# early_stopping = tf.keras.callbacks.EarlyStopping(
#     monitor='val_loss',  # metric to monitor
#     patience=3,          # number of epochs with no improvement after which training will be stopped
#     restore_best_weights=True  # restore the model weights from the epoch with the best value of the monitored metric
# )

# Training
history = model.fit(
    train_dataset.shuffle(25).batch(8),
    epochs=25,
    validation_data=val_dataset.batch(8),
    # callbacks=[early_stopping]
)

import matplotlib.pyplot as plt
fig , ax = plt.subplots(1,2)
train_acc = history.history['accuracy']
train_loss = history.history['loss']
fig.set_size_inches(12,4)

ax[0].plot(history.history['accuracy'])
ax[0].plot(history.history['val_accuracy'])
ax[0].set_title('Training Accuracy vs Validation Accuracy')
ax[0].set_ylabel('Accuracy')
ax[0].set_xlabel('Epoch')
ax[0].legend(['Train', 'Validation'], loc='upper left')

ax[1].plot(history.history['loss'])
ax[1].plot(history.history['val_loss'])
ax[1].set_title('Training Loss vs Validation Loss')
ax[1].set_ylabel('Loss')
ax[1].set_xlabel('Epoch')
ax[1].legend(['Train', 'Validation'], loc='upper left')

plt.show()

# Evaluasi Model
loss, accuracy = model.evaluate(test_dataset.batch(16))
print("Akurasi:", accuracy)
print("Loss:", loss)

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf

# Prediksi Model
predictions = model.predict(test_dataset.batch(16))
predicted_classes = tf.argmax(predictions.logits, axis=1)

# Membuat confusion matrix
conf_mat = confusion_matrix(test_labels, predicted_classes)

print("Confusion Matrix:")
print(conf_mat)

# Menampilkan laporan klasifikasi
target_names = ['Human generated', 'GPT generated']
print(classification_report(test_labels, predicted_classes, target_names=target_names))

# Membuat heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix Heatmap')
plt.show()

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Misalnya test_texts, test_labels, dan predicted_classes sudah didefinisikan sebelumnya


# Membuat DataFrame
results_df = pd.DataFrame({
    'Text': test_texts,
    'True Label': test_labels,
    'Predicted Label': predicted_classes
})

# Mengubah nilai 0 dan 1 menjadi "human generated" dan "gpt generated"
label_map = {0: "human generated", 1: "gpt generated"}
results_df['True Label'] = results_df['True Label'].map(label_map)
results_df['Predicted Label'] = results_df['Predicted Label'].map(label_map)

results_df.head()